# Manual Passo a Passo: Configura√ß√£o do Airflow para Orquestrar Jobs no Databricks

Este manual explica como configurar o Apache Airflow para orquestrar jobs no Databricks.

---

## ‚úÖ Pr√©-requisitos

* Ambiente Airflow configurado e em execu√ß√£o.
* Acesso a um workspace do Databricks (Azure Databricks, AWS Databricks, etc.).
* Permiss√µes para gerar tokens de acesso (PAT - Personal Access Token) no Databricks.

---

## üß© Passo 1: Gerar um Token de Acesso (PAT) no Databricks

1. Acesse o Databricks Workspace.
2. No canto superior direito, clique em **Settings**.<br>
![](https://github.com/Adrianogvs/airflow-databricks-orchestration/blob/main/img/01.png)

4. V√° para `User Settings ‚Üí Developer ‚Üí Access Tokens`.<br>
![](https://github.com/Adrianogvs/airflow-databricks-orchestration/blob/main/img/02.png)

5. Clique em **Generate New Token**.<br>
![](https://github.com/Adrianogvs/airflow-databricks-orchestration/blob/main/img/03.png)

6. Adicione uma descri√ß√£o (ex: `Airflow Integration`) e defina um tempo de expira√ß√£o.

7. Clique em **Generate** e **copie o token** (ele n√£o ser√° exibido novamente).<br>
![](https://github.com/Adrianogvs/airflow-databricks-orchestration/blob/main/img/04.png)

8. Salve o token em um local seguro (ser√° usado no Airflow).

---

## üß© Passo 2: Instalar o Provedor de Conex√£o do Databricks no Airflow

O Airflow requer um pacote adicional para se conectar ao Databricks.

No terminal do servidor onde o Airflow est√° instalado, execute:

```bash
pip install apache-airflow-providers-databricks
```

Ap√≥s a instala√ß√£o, reinicie os servi√ßos do Airflow:

```bash
airflow webserver --stop  
airflow scheduler --stop  
airflow webserver --start  
airflow scheduler --start  
```
### Passo alternativo - Instala√ß√£o do Provedor de Conex√£o do Databricks no Airflow

1. Configure os containers no Docker. Ajuste os tr√™s objetos conforme a imagem abaixo:<br>
![](https://github.com/Adrianogvs/airflow-databricks-orchestration/blob/main/img/05.png)

2. Clique nos tres pontinhos e selecione a op√ß√£o **Open in terminal**. <br>
![](https://github.com/Adrianogvs/airflow-databricks-orchestration/blob/main/img/06.png)

3. Ir√° abrir o terminal do Docker, cole o c√≥digo ```ip install apache-airflow-providers-databricks``` e logo ap√≥s pressione **Enter**.<br>
![](https://github.com/Adrianogvs/airflow-databricks-orchestration/blob/main/img/08.png)

4. Repita o processo para os demais containers.<br>
   II. airflow-worker-1<br>
   III. airflow-scheduler-1
---

## üß© Passo 3: Configurar a Conex√£o do Airflow com o Databricks

1. Acesse a interface do Airflow (ex: `http://localhost:8080`).

2. V√° em **Admin ‚Üí Connections**.<br>
![](https://github.com/Adrianogvs/airflow-databricks-orchestration/blob/main/img/10.png)

3. Clique em **+ (Adicionar Nova Conex√£o)**.<br>
![](https://github.com/Adrianogvs/airflow-databricks-orchestration/blob/main/img/11.png)

4. Preencha os campos:

   * **Connection Id**: `databricks_default` (ou um nome personalizado).
   * **Connection Type**: `Databricks`.
   * **Host**: URL do seu workspace Databricks (ex: `https://<seu-workspace>.cloud.databricks.com`).
   * **Password**: Cole o PAT (Token de Acesso) gerado no Passo 1.<br>
   
    ![](https://github.com/Adrianogvs/airflow-databricks-orchestration/blob/main/img/14.png)

5. Clique em **Save**.

---

## üß© Passo 4: Criar um DAG no Airflow para Disparar Jobs no Databricks

1. V√° para a pasta `dags/` no diret√≥rio de instala√ß√£o do Airflow.
2. Crie um novo arquivo Python (ex: `databricks_dag.py`).
3. Use o seguinte c√≥digo como base:

```python
from airflow import DAG
from airflow.providers.databricks.operators.databricks import DatabricksRunNowOperator
from airflow.utils.dates import days_ago

default_args = {
    'owner': 'airflow'
}

with DAG('databricks_dag',
    start_date = days_ago(2),
    schedule_interval = None,
    default_args = default_args
) as dag:
    
    opr_run_now = DatabricksRunNowOperator(
        task_id = 'run_now',
        databricks_conn_id = 'databricks', # Nome do Conn Id da List Connection
        job_id = <ID_TOKENS_ACCESS> # Access tokens gerado no Databricks 
    )

```

---

## üß© Passo 5: Testar e Executar o DAG

1. Salve o arquivo na pasta `dags/`.
2. No painel do Airflow, atualize a lista de DAGs.
3. Localize o DAG `ddatabricks_dag` e ative-o.
4. Execute manualmente clicando em **Trigger DAG**.
5. Verifique o status no Databricks (Jobs ‚Üí Runs).<br>
![](https://github.com/Adrianogvs/airflow-databricks-orchestration/blob/main/img/15.png)

---

## üìÖ Configura√ß√£o Avan√ßada

### Agendamento Autom√°tico

Modifique o `schedule_interval` no DAG para disparar automaticamente:

* `@daily` ‚Üí Todos os dias.
* `0 0 * * 0` ‚Üí Todo domingo √† meia-noite (cron syntax).

### Par√¢metros Din√¢micos

Use `{{ ds }}` (data de execu√ß√£o) ou vari√°veis do Airflow para passar par√¢metros ao notebook.

---

## ‚ö†Ô∏è Solu√ß√£o de Problemas

* **Erro "Databricks connection type not found"**: Verifique se o provider foi instalado corretamente e reinicie o Airflow.
* **Falha na autentica√ß√£o**: Confira se o PAT est√° correto e n√£o expirou.
* **Cluster indispon√≠vel**: Verifique se o cluster est√° ativo no Databricks.

---

## üöÄ Conclus√£o

Agora o Airflow est√° configurado para orquestrar jobs no Databricks, seja manualmente ou de forma agendada.

> üìå **Dica**: Sempre revogue tokens n√£o utilizados no Databricks por seguran√ßa.

> üîó **Refer√™ncia**: Documenta√ß√£o do Airflow-Databricks

### Pr√≥ximos passos:

* Automatizar pipelines mais complexos com m√∫ltiplos jobs.
* Usar Databricks Jobs API para maior controle.
* Configurar alertas de falhas no Airflow.

Espero que este manual seja √∫til! üöÄ
