# Manual Passo a Passo: Configura√ß√£o do Airflow para Orquestrar Jobs no Databricks

Este manual explica como configurar o Apache Airflow para orquestrar jobs no Databricks.

---

## ‚úÖ Pr√©-requisitos

* Ambiente Airflow configurado e em execu√ß√£o.
* Acesso a um workspace do Databricks (Azure Databricks, AWS Databricks, etc.).
* Permiss√µes para gerar tokens de acesso (PAT - Personal Access Token) no Databricks.

---

## üß© Passo 1: Gerar um Token de Acesso (PAT) no Databricks

1. Acesse o Databricks Workspace.
2. No canto superior direito, clique em **Settings**.

4. V√° para `User Settings ‚Üí Developer ‚Üí Access Tokens`.
5. Clique em **Generate New Token**.
6. Adicione uma descri√ß√£o (ex: `Airflow Integration`) e defina um tempo de expira√ß√£o.
7. Clique em **Generate** e **copie o token** (ele n√£o ser√° exibido novamente).
8. Salve o token em um local seguro (ser√° usado no Airflow).

---

## üß© Passo 2: Instalar o Provedor de Conex√£o do Databricks no Airflow

O Airflow requer um pacote adicional para se conectar ao Databricks.

No terminal do servidor onde o Airflow est√° instalado, execute:

```bash
pip install apache-airflow-providers-databricks
```

Ap√≥s a instala√ß√£o, reinicie os servi√ßos do Airflow:

```bash
airflow webserver --stop  
airflow scheduler --stop  
airflow webserver --start  
airflow scheduler --start  
```

---

## üß© Passo 3: Configurar a Conex√£o do Airflow com o Databricks

1. Acesse a interface do Airflow (ex: `http://localhost:8080`).

2. V√° em **Admin ‚Üí Connections**.

3. Clique em **+ (Adicionar Nova Conex√£o)**.

4. Preencha os campos:

   * **Connection Id**: `databricks_default` (ou um nome personalizado).
   * **Connection Type**: `Databricks`.
   * **Host**: URL do seu workspace Databricks (ex: `https://<seu-workspace>.cloud.databricks.com`).
   * **Password**: Cole o PAT (Token de Acesso) gerado no Passo 1.

5. Clique em **Save**.

---

## üß© Passo 4: Criar um DAG no Airflow para Disparar Jobs no Databricks

1. V√° para a pasta `dags/` no diret√≥rio de instala√ß√£o do Airflow.
2. Crie um novo arquivo Python (ex: `databricks_job_trigger.py`).
3. Use o seguinte c√≥digo como base:

```python
from airflow import DAG
from airflow.providers.databricks.operators.databricks import DatabricksSubmitRunOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2023, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    'databricks_job_trigger',
    default_args=default_args,
    schedule_interval=None,  # Pode ser definido (ex: '@daily')
    catchup=False,
) as dag:

    submit_job = DatabricksSubmitRunOperator(
        task_id='submit_databricks_job',
        databricks_conn_id='databricks_default',  # Nome da conex√£o criada
        existing_cluster_id='<ID_DO_CLUSTER>',  # Ou use `new_cluster` para criar um novo
        notebook_task={
            'notebook_path': '/caminho/do/seu/notebook',
        },
        # Alternativamente, use `spark_python_task` ou `spark_jar_task`
    )

    submit_job
```

### Onde obter os par√¢metros?

* `existing_cluster_id`: Encontre no Databricks em **Clusters ‚Üí Selecione o cluster ‚Üí Configuration**.
* `notebook_path`: Caminho do notebook no Databricks (ex: `/Users/seu_usuario/meu_notebook`).

---

## üß© Passo 5: Testar e Executar o DAG

1. Salve o arquivo na pasta `dags/`.
2. No painel do Airflow, atualize a lista de DAGs.
3. Localize o DAG `databricks_job_trigger` e ative-o.
4. Execute manualmente clicando em **Trigger DAG**.
5. Verifique o status no Databricks (Jobs ‚Üí Runs).

---

## üìÖ Configura√ß√£o Avan√ßada

### Agendamento Autom√°tico

Modifique o `schedule_interval` no DAG para disparar automaticamente:

* `@daily` ‚Üí Todos os dias.
* `0 0 * * 0` ‚Üí Todo domingo √† meia-noite (cron syntax).

### Par√¢metros Din√¢micos

Use `{{ ds }}` (data de execu√ß√£o) ou vari√°veis do Airflow para passar par√¢metros ao notebook.

---

## ‚ö†Ô∏è Solu√ß√£o de Problemas

* **Erro "Databricks connection type not found"**: Verifique se o provider foi instalado corretamente e reinicie o Airflow.
* **Falha na autentica√ß√£o**: Confira se o PAT est√° correto e n√£o expirou.
* **Cluster indispon√≠vel**: Verifique se o cluster est√° ativo no Databricks.

---

## üöÄ Conclus√£o

Agora o Airflow est√° configurado para orquestrar jobs no Databricks, seja manualmente ou de forma agendada.

> üìå **Dica**: Sempre revogue tokens n√£o utilizados no Databricks por seguran√ßa.

> üîó **Refer√™ncia**: Documenta√ß√£o do Airflow-Databricks

### Pr√≥ximos passos:

* Automatizar pipelines mais complexos com m√∫ltiplos jobs.
* Usar Databricks Jobs API para maior controle.
* Configurar alertas de falhas no Airflow.

Espero que este manual seja √∫til! üöÄ


[def]: D:\Documents\GitHub\project-with-airflow-docker\img\01.png
